2025-12-04 19:15:21,570 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 19:15:21,570 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 19:15:21,910 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=0, curr_reserved_bytes=0, max_allocated_bytes=0, max_reserved_bytes=0)
2025-12-04 19:15:22,899 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=763973120, curr_reserved_bytes=815792128, max_allocated_bytes=763973120, max_reserved_bytes=815792128), peak=MemoryStats(curr_allocated_bytes=3328421888, curr_reserved_bytes=4617928704, max_allocated_bytes=4067974656, max_reserved_bytes=4617928704)
2025-12-04 19:15:22,901 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 19:15:22,901 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 19:15:22,902 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=4617928704, max_allocated_bytes=3328421888, max_reserved_bytes=4617928704)
2025-12-04 19:15:23,252 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=808393216, curr_reserved_bytes=4617928704, max_allocated_bytes=808393216, max_reserved_bytes=4617928704), peak=MemoryStats(curr_allocated_bytes=3343350784, curr_reserved_bytes=4622123008, max_allocated_bytes=3808643072, max_reserved_bytes=4622123008)
2025-12-04 19:15:23,254 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 19:15:23,254 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 19:15:23,254 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=4622123008, max_allocated_bytes=3343350784, max_reserved_bytes=4622123008)
2025-12-04 19:15:23,628 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=808393216, curr_reserved_bytes=4622123008, max_allocated_bytes=808393216, max_reserved_bytes=4622123008), peak=MemoryStats(curr_allocated_bytes=3341791232, curr_reserved_bytes=4622123008, max_allocated_bytes=3807332352, max_reserved_bytes=4622123008)
2025-12-04 19:15:23,630 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 19:15:23,630 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 19:15:23,631 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=4622123008, max_allocated_bytes=3341791232, max_reserved_bytes=4622123008)
2025-12-04 19:15:23,949 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=808393216, curr_reserved_bytes=4622123008, max_allocated_bytes=808393216, max_reserved_bytes=4622123008), peak=MemoryStats(curr_allocated_bytes=3352028160, curr_reserved_bytes=4829741056, max_allocated_bytes=4628317696, max_reserved_bytes=4829741056)
2025-12-04 19:15:23,951 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 19:15:23,951 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 19:15:23,952 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=4829741056, max_allocated_bytes=3352028160, max_reserved_bytes=4829741056)
2025-12-04 19:15:26,106 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4308719104, curr_reserved_bytes=5345640448, max_allocated_bytes=4308719104, max_reserved_bytes=5345640448), peak=MemoryStats(curr_allocated_bytes=17664312320, curr_reserved_bytes=21021851648, max_allocated_bytes=18569643520, max_reserved_bytes=21021851648)
2025-12-04 19:15:26,109 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 19:15:26,109 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 19:15:26,110 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=21021851648, max_allocated_bytes=17664312320, max_reserved_bytes=21021851648)
2025-12-04 19:15:28,321 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4421965312, curr_reserved_bytes=21021851648, max_allocated_bytes=4421965312, max_reserved_bytes=21021851648), peak=MemoryStats(curr_allocated_bytes=17665360896, curr_reserved_bytes=21034434560, max_allocated_bytes=18442880000, max_reserved_bytes=21034434560)
2025-12-04 19:15:28,325 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 19:15:28,325 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 19:15:28,326 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=21034434560, max_allocated_bytes=17665360896, max_reserved_bytes=21034434560)
2025-12-04 19:15:30,600 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4421965312, curr_reserved_bytes=21034434560, max_allocated_bytes=4421965312, max_reserved_bytes=21034434560), peak=MemoryStats(curr_allocated_bytes=17667982336, curr_reserved_bytes=21034434560, max_allocated_bytes=18442880000, max_reserved_bytes=21034434560)
2025-12-04 19:15:30,602 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 19:15:30,602 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 19:15:30,603 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=21034434560, max_allocated_bytes=17667982336, max_reserved_bytes=21034434560)
2025-12-04 19:15:32,483 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4421965312, curr_reserved_bytes=21034434560, max_allocated_bytes=4421965312, max_reserved_bytes=21034434560), peak=MemoryStats(curr_allocated_bytes=17641767936, curr_reserved_bytes=22689087488, max_allocated_bytes=21251720704, max_reserved_bytes=22689087488)
