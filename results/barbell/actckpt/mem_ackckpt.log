2025-12-04 17:53:37,748 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 17:53:37,748 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 17:53:39,049 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=763973120, curr_reserved_bytes=815792128, max_allocated_bytes=763973120, max_reserved_bytes=815792128), peak=MemoryStats(curr_allocated_bytes=3328421888, curr_reserved_bytes=4617928704, max_allocated_bytes=4067974656, max_reserved_bytes=4617928704)
2025-12-04 17:53:39,051 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 17:53:39,051 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 17:53:39,399 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=808393216, curr_reserved_bytes=4617928704, max_allocated_bytes=3328421888, max_reserved_bytes=4617928704), peak=MemoryStats(curr_allocated_bytes=3343350784, curr_reserved_bytes=4622123008, max_allocated_bytes=3808643072, max_reserved_bytes=4622123008)
2025-12-04 17:53:39,401 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 17:53:39,401 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 17:53:39,777 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=808393216, curr_reserved_bytes=4622123008, max_allocated_bytes=3343350784, max_reserved_bytes=4622123008), peak=MemoryStats(curr_allocated_bytes=3341791232, curr_reserved_bytes=4622123008, max_allocated_bytes=3807332352, max_reserved_bytes=4622123008)
2025-12-04 17:53:39,778 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 17:53:39,778 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=small, model_config={'vocab_size': 50257, 'num_layers': 12, 'd_model': 768, 'num_heads': 12, 'd_ff': 3072}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 17:53:40,097 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=808393216, curr_reserved_bytes=4622123008, max_allocated_bytes=3341791232, max_reserved_bytes=4622123008), peak=MemoryStats(curr_allocated_bytes=3352028160, curr_reserved_bytes=4829741056, max_allocated_bytes=4628317696, max_reserved_bytes=4829741056)
2025-12-04 17:53:40,099 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 17:53:40,099 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 17:53:42,255 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4308719104, curr_reserved_bytes=5345640448, max_allocated_bytes=4308719104, max_reserved_bytes=5345640448), peak=MemoryStats(curr_allocated_bytes=17664312320, curr_reserved_bytes=21021851648, max_allocated_bytes=18569643520, max_reserved_bytes=21021851648)
2025-12-04 17:53:42,259 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 17:53:42,259 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 17:53:44,397 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4421965312, curr_reserved_bytes=21021851648, max_allocated_bytes=17664312320, max_reserved_bytes=21021851648), peak=MemoryStats(curr_allocated_bytes=17665360896, curr_reserved_bytes=21034434560, max_allocated_bytes=18442880000, max_reserved_bytes=21034434560)
2025-12-04 17:53:44,402 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 17:53:44,402 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 17:53:46,686 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4421965312, curr_reserved_bytes=21034434560, max_allocated_bytes=17665360896, max_reserved_bytes=21034434560), peak=MemoryStats(curr_allocated_bytes=17667982336, curr_reserved_bytes=21034434560, max_allocated_bytes=18442880000, max_reserved_bytes=21034434560)
2025-12-04 17:53:46,689 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 17:53:46,689 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large, model_config={'vocab_size': 50257, 'num_layers': 36, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 17:53:48,575 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4421965312, curr_reserved_bytes=21034434560, max_allocated_bytes=17667982336, max_reserved_bytes=21034434560), peak=MemoryStats(curr_allocated_bytes=17641767936, curr_reserved_bytes=22689087488, max_allocated_bytes=21251720704, max_reserved_bytes=22689087488)
