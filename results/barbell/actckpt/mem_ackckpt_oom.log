2025-12-04 20:35:51,280 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:35:51,280 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/40layers, model_config={'vocab_size': 50257, 'num_layers': 40, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:35:51,658 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=0, curr_reserved_bytes=0, max_allocated_bytes=0, max_reserved_bytes=0)
2025-12-04 20:35:54,571 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4836980224, curr_reserved_bytes=4922015744, max_allocated_bytes=4836980224, max_reserved_bytes=4922015744), peak=MemoryStats(curr_allocated_bytes=19506955264, curr_reserved_bytes=23047700480, max_allocated_bytes=20445147648, max_reserved_bytes=23047700480)
2025-12-04 20:35:54,576 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:35:54,576 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/40layers, model_config={'vocab_size': 50257, 'num_layers': 40, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:35:54,577 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=23047700480, max_allocated_bytes=19506955264, max_reserved_bytes=23047700480)
2025-12-04 20:35:56,922 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4854674944, curr_reserved_bytes=23047700480, max_allocated_bytes=4854674944, max_reserved_bytes=23047700480), peak=MemoryStats(curr_allocated_bytes=19505906688, curr_reserved_bytes=23064477696, max_allocated_bytes=20288144384, max_reserved_bytes=23064477696)
2025-12-04 20:35:56,929 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:35:56,929 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/40layers, model_config={'vocab_size': 50257, 'num_layers': 40, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:35:56,930 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=23064477696, max_allocated_bytes=19505906688, max_reserved_bytes=23064477696)
2025-12-04 20:35:59,535 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4854674944, curr_reserved_bytes=23064477696, max_allocated_bytes=4854674944, max_reserved_bytes=23064477696), peak=MemoryStats(curr_allocated_bytes=19513246720, curr_reserved_bytes=23064477696, max_allocated_bytes=20288144384, max_reserved_bytes=23064477696)
2025-12-04 20:35:59,538 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:35:59,538 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/40layers, model_config={'vocab_size': 50257, 'num_layers': 40, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:35:59,538 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=23064477696, max_allocated_bytes=19513246720, max_reserved_bytes=23064477696)
2025-12-04 20:36:01,646 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4854674944, curr_reserved_bytes=23064477696, max_allocated_bytes=4854674944, max_reserved_bytes=23064477696), peak=MemoryStats(curr_allocated_bytes=19489129472, curr_reserved_bytes=21674065920, max_allocated_bytes=23434670592, max_reserved_bytes=24719130624)
2025-12-04 20:36:01,651 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:01,651 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/41layers, model_config={'vocab_size': 50257, 'num_layers': 41, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:01,652 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=21674065920, max_allocated_bytes=19489129472, max_reserved_bytes=21674065920)
2025-12-04 20:36:03,954 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4962688512, curr_reserved_bytes=21674065920, max_allocated_bytes=4962688512, max_reserved_bytes=21674065920), peak=MemoryStats(curr_allocated_bytes=19894969344, curr_reserved_bytes=22189965312, max_allocated_bytes=20891875840, max_reserved_bytes=22189965312)
2025-12-04 20:36:03,957 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:36:03,957 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/41layers, model_config={'vocab_size': 50257, 'num_layers': 41, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:36:03,958 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22189965312, max_allocated_bytes=19894969344, max_reserved_bytes=22189965312)
2025-12-04 20:36:06,363 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4962688512, curr_reserved_bytes=22189965312, max_allocated_bytes=4962688512, max_reserved_bytes=22189965312), peak=MemoryStats(curr_allocated_bytes=19886580736, curr_reserved_bytes=22196256768, max_allocated_bytes=20682449920, max_reserved_bytes=22196256768)
2025-12-04 20:36:06,367 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:36:06,367 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/41layers, model_config={'vocab_size': 50257, 'num_layers': 41, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:36:06,368 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22196256768, max_allocated_bytes=19886580736, max_reserved_bytes=22196256768)
2025-12-04 20:36:08,951 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4962688512, curr_reserved_bytes=22196256768, max_allocated_bytes=4962688512, max_reserved_bytes=22196256768), peak=MemoryStats(curr_allocated_bytes=19912795136, curr_reserved_bytes=22196256768, max_allocated_bytes=20688741376, max_reserved_bytes=22196256768)
2025-12-04 20:36:08,953 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:36:08,953 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/41layers, model_config={'vocab_size': 50257, 'num_layers': 41, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:36:08,954 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22196256768, max_allocated_bytes=19912795136, max_reserved_bytes=22196256768)
2025-12-04 20:36:11,135 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=4962688512, curr_reserved_bytes=22196256768, max_allocated_bytes=4962688512, max_reserved_bytes=22196256768), peak=MemoryStats(curr_allocated_bytes=19828384768, curr_reserved_bytes=22322085888, max_allocated_bytes=23935322624, max_reserved_bytes=24719130624)
2025-12-04 20:36:11,139 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:11,140 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/42layers, model_config={'vocab_size': 50257, 'num_layers': 42, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:11,141 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22322085888, max_allocated_bytes=19828384768, max_reserved_bytes=22322085888)
2025-12-04 20:36:13,503 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5070702080, curr_reserved_bytes=22322085888, max_allocated_bytes=5070702080, max_reserved_bytes=22322085888), peak=MemoryStats(curr_allocated_bytes=20245234688, curr_reserved_bytes=22580035584, max_allocated_bytes=21252620800, max_reserved_bytes=22580035584)
2025-12-04 20:36:13,506 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:36:13,506 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/42layers, model_config={'vocab_size': 50257, 'num_layers': 42, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:36:13,507 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22580035584, max_allocated_bytes=20245234688, max_reserved_bytes=22580035584)
2025-12-04 20:36:15,975 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5070702080, curr_reserved_bytes=22580035584, max_allocated_bytes=5070702080, max_reserved_bytes=22580035584), peak=MemoryStats(curr_allocated_bytes=20225311744, curr_reserved_bytes=22590521344, max_allocated_bytes=21022229504, max_reserved_bytes=22590521344)
2025-12-04 20:36:15,979 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:36:15,980 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/42layers, model_config={'vocab_size': 50257, 'num_layers': 42, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:36:15,980 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22590521344, max_allocated_bytes=20225311744, max_reserved_bytes=22590521344)
2025-12-04 20:36:18,720 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5070702080, curr_reserved_bytes=22590521344, max_allocated_bytes=5070702080, max_reserved_bytes=22590521344), peak=MemoryStats(curr_allocated_bytes=20251526144, curr_reserved_bytes=22590521344, max_allocated_bytes=21028520960, max_reserved_bytes=22590521344)
2025-12-04 20:36:18,722 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:36:18,723 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/42layers, model_config={'vocab_size': 50257, 'num_layers': 42, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:36:18,723 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22590521344, max_allocated_bytes=20251526144, max_reserved_bytes=22590521344)
2025-12-04 20:36:21,008 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5070702080, curr_reserved_bytes=22590521344, max_allocated_bytes=5070702080, max_reserved_bytes=22590521344), peak=MemoryStats(curr_allocated_bytes=20191233024, curr_reserved_bytes=22739419136, max_allocated_bytes=24379351552, max_reserved_bytes=24935137280)
2025-12-04 20:36:21,013 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:21,013 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/43layers, model_config={'vocab_size': 50257, 'num_layers': 43, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:21,014 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22739419136, max_allocated_bytes=20191233024, max_reserved_bytes=22739419136)
2025-12-04 20:36:23,433 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5178715648, curr_reserved_bytes=22739419136, max_allocated_bytes=5178715648, max_reserved_bytes=22739419136), peak=MemoryStats(curr_allocated_bytes=20611752960, curr_reserved_bytes=22997368832, max_allocated_bytes=21634337280, max_reserved_bytes=22997368832)
2025-12-04 20:36:23,435 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:36:23,436 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/43layers, model_config={'vocab_size': 50257, 'num_layers': 43, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:36:23,436 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=22997368832, max_allocated_bytes=20611752960, max_reserved_bytes=22997368832)
2025-12-04 20:36:25,966 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5178715648, curr_reserved_bytes=22997368832, max_allocated_bytes=5178715648, max_reserved_bytes=22997368832), peak=MemoryStats(curr_allocated_bytes=20589208576, curr_reserved_bytes=23007854592, max_allocated_bytes=21382980608, max_reserved_bytes=23007854592)
2025-12-04 20:36:25,971 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:36:25,971 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/43layers, model_config={'vocab_size': 50257, 'num_layers': 43, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:36:25,972 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=23007854592, max_allocated_bytes=20589208576, max_reserved_bytes=23007854592)
2025-12-04 20:36:28,688 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5178715648, curr_reserved_bytes=23007854592, max_allocated_bytes=5178715648, max_reserved_bytes=23007854592), peak=MemoryStats(curr_allocated_bytes=20615422976, curr_reserved_bytes=23007854592, max_allocated_bytes=21391369216, max_reserved_bytes=23007854592)
2025-12-04 20:36:28,690 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:36:28,690 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/43layers, model_config={'vocab_size': 50257, 'num_layers': 43, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:36:28,691 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=23007854592, max_allocated_bytes=20615422976, max_reserved_bytes=23007854592)
2025-12-04 20:36:30,061 __main__ INFO OOM? CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 57.88 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 261.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:36:30,069 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:30,070 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/44layers, model_config={'vocab_size': 50257, 'num_layers': 44, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:30,105 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24912068608, max_allocated_bytes=24637463040, max_reserved_bytes=24912068608)
2025-12-04 20:36:32,581 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5286729216, curr_reserved_bytes=24912068608, max_allocated_bytes=5286729216, max_reserved_bytes=24912068608), peak=MemoryStats(curr_allocated_bytes=21032272896, curr_reserved_bytes=24912068608, max_allocated_bytes=22047511040, max_reserved_bytes=24912068608)
2025-12-04 20:36:32,584 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:36:32,584 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/44layers, model_config={'vocab_size': 50257, 'num_layers': 44, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:36:32,585 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24912068608, max_allocated_bytes=21032272896, max_reserved_bytes=24912068608)
2025-12-04 20:36:35,174 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5286729216, curr_reserved_bytes=24912068608, max_allocated_bytes=5286729216, max_reserved_bytes=24912068608), peak=MemoryStats(curr_allocated_bytes=21021262848, curr_reserved_bytes=24912068608, max_allocated_bytes=21810840576, max_reserved_bytes=24912068608)
2025-12-04 20:36:35,180 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:36:35,180 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/44layers, model_config={'vocab_size': 50257, 'num_layers': 44, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:36:35,181 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24912068608, max_allocated_bytes=21021262848, max_reserved_bytes=24912068608)
2025-12-04 20:36:37,960 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5286729216, curr_reserved_bytes=24912068608, max_allocated_bytes=5286729216, max_reserved_bytes=24912068608), peak=MemoryStats(curr_allocated_bytes=21035942912, curr_reserved_bytes=24912068608, max_allocated_bytes=21810840576, max_reserved_bytes=24912068608)
2025-12-04 20:36:37,963 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:36:37,963 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/44layers, model_config={'vocab_size': 50257, 'num_layers': 44, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:36:37,964 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24912068608, max_allocated_bytes=21035942912, max_reserved_bytes=24912068608)
2025-12-04 20:36:39,316 __main__ INFO OOM? CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 17.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.97 GiB is allocated by PyTorch, and 273.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:36:39,324 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:39,326 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/45layers, model_config={'vocab_size': 50257, 'num_layers': 45, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:39,397 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24954011648, max_allocated_bytes=24667656192, max_reserved_bytes=24954011648)
2025-12-04 20:36:41,928 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5394742784, curr_reserved_bytes=24954011648, max_allocated_bytes=5394742784, max_reserved_bytes=24954011648), peak=MemoryStats(curr_allocated_bytes=21450695680, curr_reserved_bytes=24954011648, max_allocated_bytes=22495287808, max_reserved_bytes=24954011648)
2025-12-04 20:36:41,931 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:36:41,931 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/45layers, model_config={'vocab_size': 50257, 'num_layers': 45, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:36:41,932 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24954011648, max_allocated_bytes=21450695680, max_reserved_bytes=24954011648)
2025-12-04 20:36:44,667 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5394742784, curr_reserved_bytes=24954011648, max_allocated_bytes=5394742784, max_reserved_bytes=24954011648), peak=MemoryStats(curr_allocated_bytes=21440734208, curr_reserved_bytes=24954011648, max_allocated_bytes=22230311936, max_reserved_bytes=24954011648)
2025-12-04 20:36:44,672 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:36:44,672 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/45layers, model_config={'vocab_size': 50257, 'num_layers': 45, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:36:44,673 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24954011648, max_allocated_bytes=21440734208, max_reserved_bytes=24954011648)
2025-12-04 20:36:47,520 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5394742784, curr_reserved_bytes=24954011648, max_allocated_bytes=5394742784, max_reserved_bytes=24954011648), peak=MemoryStats(curr_allocated_bytes=21455414272, curr_reserved_bytes=24954011648, max_allocated_bytes=22230311936, max_reserved_bytes=24954011648)
2025-12-04 20:36:47,523 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:36:47,523 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/45layers, model_config={'vocab_size': 50257, 'num_layers': 45, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:36:47,524 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24954011648, max_allocated_bytes=21455414272, max_reserved_bytes=24954011648)
2025-12-04 20:36:48,898 __main__ INFO OOM? CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 17.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 263.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:36:48,906 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:48,907 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/46layers, model_config={'vocab_size': 50257, 'num_layers': 46, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:48,980 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24954011648, max_allocated_bytes=24684224512, max_reserved_bytes=24954011648)
2025-12-04 20:36:51,601 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5502756352, curr_reserved_bytes=24954011648, max_allocated_bytes=5502756352, max_reserved_bytes=24954011648), peak=MemoryStats(curr_allocated_bytes=21872264192, curr_reserved_bytes=23871881216, max_allocated_bytes=22945161728, max_reserved_bytes=24954011648)
2025-12-04 20:36:51,604 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:36:51,604 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/46layers, model_config={'vocab_size': 50257, 'num_layers': 46, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:36:51,605 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=23871881216, max_allocated_bytes=21872264192, max_reserved_bytes=23871881216)
2025-12-04 20:36:54,317 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5502756352, curr_reserved_bytes=23871881216, max_allocated_bytes=5502756352, max_reserved_bytes=23871881216), peak=MemoryStats(curr_allocated_bytes=21848671232, curr_reserved_bytes=24150802432, max_allocated_bytes=22640346112, max_reserved_bytes=24150802432)
2025-12-04 20:36:54,322 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:36:54,322 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/46layers, model_config={'vocab_size': 50257, 'num_layers': 46, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:36:54,323 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24150802432, max_allocated_bytes=21848671232, max_reserved_bytes=24150802432)
2025-12-04 20:36:57,234 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5502756352, curr_reserved_bytes=24150802432, max_allocated_bytes=5502756352, max_reserved_bytes=24150802432), peak=MemoryStats(curr_allocated_bytes=21872788480, curr_reserved_bytes=24150802432, max_allocated_bytes=22649783296, max_reserved_bytes=24150802432)
2025-12-04 20:36:57,237 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:36:57,237 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/46layers, model_config={'vocab_size': 50257, 'num_layers': 46, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:36:57,238 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24150802432, max_allocated_bytes=21872788480, max_reserved_bytes=24150802432)
2025-12-04 20:36:58,712 __main__ INFO OOM? CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 5.88 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 295.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:36:58,721 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:36:58,723 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/47layers, model_config={'vocab_size': 50257, 'num_layers': 47, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:36:58,724 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24966594560, max_allocated_bytes=24672720896, max_reserved_bytes=24968691712)
2025-12-04 20:37:01,432 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5610769920, curr_reserved_bytes=24966594560, max_allocated_bytes=5610769920, max_reserved_bytes=24966594560), peak=MemoryStats(curr_allocated_bytes=22289114112, curr_reserved_bytes=24255660032, max_allocated_bytes=23416007168, max_reserved_bytes=24966594560)
2025-12-04 20:37:01,436 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:37:01,436 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/47layers, model_config={'vocab_size': 50257, 'num_layers': 47, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:37:01,437 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24255660032, max_allocated_bytes=22289114112, max_reserved_bytes=24255660032)
2025-12-04 20:37:04,263 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5610769920, curr_reserved_bytes=24255660032, max_allocated_bytes=5610769920, max_reserved_bytes=24255660032), peak=MemoryStats(curr_allocated_bytes=22261851136, curr_reserved_bytes=24792530944, max_allocated_bytes=23055623168, max_reserved_bytes=24792530944)
2025-12-04 20:37:04,269 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:37:04,269 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/47layers, model_config={'vocab_size': 50257, 'num_layers': 47, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:37:04,270 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24792530944, max_allocated_bytes=22261851136, max_reserved_bytes=24792530944)
2025-12-04 20:37:07,246 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5610769920, curr_reserved_bytes=24792530944, max_allocated_bytes=5610769920, max_reserved_bytes=24792530944), peak=MemoryStats(curr_allocated_bytes=22293308416, curr_reserved_bytes=24792530944, max_allocated_bytes=23068206080, max_reserved_bytes=24792530944)
2025-12-04 20:37:07,250 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:37:07,250 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/47layers, model_config={'vocab_size': 50257, 'num_layers': 47, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:37:07,251 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24792530944, max_allocated_bytes=22293308416, max_reserved_bytes=24792530944)
2025-12-04 20:37:08,744 __main__ INFO OOM? CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 15.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 287.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:08,758 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:37:08,759 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/48layers, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:37:08,760 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24956108800, max_allocated_bytes=24654485504, max_reserved_bytes=24960303104)
2025-12-04 20:37:11,487 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5718783488, curr_reserved_bytes=24956108800, max_allocated_bytes=5718783488, max_reserved_bytes=24956108800), peak=MemoryStats(curr_allocated_bytes=22711206912, curr_reserved_bytes=24383586304, max_allocated_bytes=23853298176, max_reserved_bytes=24956108800)
2025-12-04 20:37:11,491 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:37:11,491 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/48layers, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:37:11,492 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24383586304, max_allocated_bytes=22711206912, max_reserved_bytes=24383586304)
2025-12-04 20:37:14,325 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5718783488, curr_reserved_bytes=24383586304, max_allocated_bytes=5718783488, max_reserved_bytes=24383586304), peak=MemoryStats(curr_allocated_bytes=22679225344, curr_reserved_bytes=24920457216, max_allocated_bytes=23469851648, max_reserved_bytes=24920457216)
2025-12-04 20:37:14,330 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:37:14,331 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/48layers, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:37:14,332 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24920457216, max_allocated_bytes=22679225344, max_reserved_bytes=24920457216)
2025-12-04 20:37:17,375 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5718783488, curr_reserved_bytes=24920457216, max_allocated_bytes=5718783488, max_reserved_bytes=24920457216), peak=MemoryStats(curr_allocated_bytes=22709634048, curr_reserved_bytes=24920457216, max_allocated_bytes=23484531712, max_reserved_bytes=24920457216)
2025-12-04 20:37:17,378 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:37:17,378 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/48layers, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:37:17,379 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24920457216, max_allocated_bytes=22709634048, max_reserved_bytes=24920457216)
2025-12-04 20:37:18,894 __main__ INFO OOM? CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 13.88 MiB is free. Including non-PyTorch memory, this process has 23.45 GiB memory in use. Of the allocated memory 22.93 GiB is allocated by PyTorch, and 317.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:18,902 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:37:18,904 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/49layers, model_config={'vocab_size': 50257, 'num_layers': 49, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:37:18,905 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24958205952, max_allocated_bytes=24625506304, max_reserved_bytes=24962400256)
2025-12-04 20:37:21,691 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5826797056, curr_reserved_bytes=24958205952, max_allocated_bytes=5826797056, max_reserved_bytes=24958205952), peak=MemoryStats(curr_allocated_bytes=23118619648, curr_reserved_bytes=24654118912, max_allocated_bytes=24278006272, max_reserved_bytes=24958205952)
2025-12-04 20:37:21,694 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:37:21,695 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/49layers, model_config={'vocab_size': 50257, 'num_layers': 49, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:37:21,696 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24654118912, max_allocated_bytes=23118619648, max_reserved_bytes=24654118912)
2025-12-04 20:37:24,672 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5826797056, curr_reserved_bytes=24654118912, max_allocated_bytes=5826797056, max_reserved_bytes=24654118912), peak=MemoryStats(curr_allocated_bytes=23090308096, curr_reserved_bytes=24593301504, max_allocated_bytes=23870448640, max_reserved_bytes=24933040128)
2025-12-04 20:37:24,677 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:37:24,677 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/49layers, model_config={'vocab_size': 50257, 'num_layers': 49, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:37:24,678 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24593301504, max_allocated_bytes=23090308096, max_reserved_bytes=24593301504)
2025-12-04 20:37:27,787 __main__ INFO Memory usage: base=MemoryStats(curr_allocated_bytes=5826797056, curr_reserved_bytes=24593301504, max_allocated_bytes=5826797056, max_reserved_bytes=24593301504), peak=MemoryStats(curr_allocated_bytes=23119668224, curr_reserved_bytes=24593301504, max_allocated_bytes=23893517312, max_reserved_bytes=24593301504)
2025-12-04 20:37:27,790 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:37:27,790 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/49layers, model_config={'vocab_size': 50257, 'num_layers': 49, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:37:27,791 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24593301504, max_allocated_bytes=23119668224, max_reserved_bytes=24593301504)
2025-12-04 20:37:29,323 __main__ INFO OOM? CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 17.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 267.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:29,333 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:37:29,333 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/50layers, model_config={'vocab_size': 50257, 'num_layers': 50, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:37:29,334 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24954011648, max_allocated_bytes=24673118208, max_reserved_bytes=24960303104)
2025-12-04 20:37:31,095 __main__ INFO OOM? CUDA out of memory. Tried to allocate 198.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 31.88 MiB is free. Including non-PyTorch memory, this process has 23.43 GiB memory in use. Of the allocated memory 22.82 GiB is allocated by PyTorch, and 417.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:31,102 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:37:31,102 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/50layers, model_config={'vocab_size': 50257, 'num_layers': 50, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:37:31,103 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24939331584, max_allocated_bytes=24501592576, max_reserved_bytes=24954011648)
2025-12-04 20:37:33,738 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 215.88 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 936.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:33,746 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:37:33,746 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/50layers, model_config={'vocab_size': 50257, 'num_layers': 50, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:37:33,747 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24746393600, max_allocated_bytes=24286774272, max_reserved_bytes=24962400256)
2025-12-04 20:37:35,383 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.40 GiB is allocated by PyTorch, and 648.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:35,386 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:37:35,386 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/50layers, model_config={'vocab_size': 50257, 'num_layers': 50, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:37:35,387 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24735907840, max_allocated_bytes=24055563264, max_reserved_bytes=24746393600)
2025-12-04 20:37:36,740 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 213.88 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 22.40 GiB is allocated by PyTorch, and 664.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:36,745 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:37:36,746 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/51layers, model_config={'vocab_size': 50257, 'num_layers': 51, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:37:36,747 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24748490752, max_allocated_bytes=24051893248, max_reserved_bytes=24756879360)
2025-12-04 20:37:38,280 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.55 GiB is allocated by PyTorch, and 494.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:38,284 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:37:38,284 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/51layers, model_config={'vocab_size': 50257, 'num_layers': 51, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:37:38,285 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24735907840, max_allocated_bytes=24217084928, max_reserved_bytes=24748490752)
2025-12-04 20:37:39,834 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 217.88 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 779.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:39,840 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:37:39,841 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/51layers, model_config={'vocab_size': 50257, 'num_layers': 51, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:37:39,841 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24744296448, max_allocated_bytes=23926629376, max_reserved_bytes=24756879360)
2025-12-04 20:37:41,505 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 225.88 MiB is free. Including non-PyTorch memory, this process has 23.24 GiB memory in use. Of the allocated memory 22.55 GiB is allocated by PyTorch, and 495.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:41,510 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:37:41,510 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=large/51layers, model_config={'vocab_size': 50257, 'num_layers': 51, 'd_model': 1280, 'num_heads': 20, 'd_ff': 5120}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:37:41,511 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24735907840, max_allocated_bytes=24216560640, max_reserved_bytes=24744296448)
2025-12-04 20:37:42,881 __main__ INFO OOM? CUDA out of memory. Tried to allocate 246.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 213.88 MiB is free. Including non-PyTorch memory, this process has 23.25 GiB memory in use. Of the allocated memory 22.31 GiB is allocated by PyTorch, and 758.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:42,889 __main__ INFO === Benchmark AttnCheckpointedTransformer memory ===
2025-12-04 20:37:42,890 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=xlarge, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1536, 'num_heads': 24, 'd_ff': 6144}, batch_size=4, seq_len=256, transformer_cls=AttnCheckpointedTransformer
2025-12-04 20:37:42,891 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24748490752, max_allocated_bytes=23952843776, max_reserved_bytes=24756879360)
2025-12-04 20:37:44,877 __main__ INFO OOM? CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 21.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:44,881 __main__ INFO === Benchmark FFNCheckpointedTransformer memory ===
2025-12-04 20:37:44,881 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=xlarge, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1536, 'num_heads': 24, 'd_ff': 6144}, batch_size=4, seq_len=256, transformer_cls=FFNCheckpointedTransformer
2025-12-04 20:37:44,907 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24949817344, max_allocated_bytes=23859819520, max_reserved_bytes=24949817344)
2025-12-04 20:37:46,608 __main__ INFO OOM? CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 1.88 MiB is free. Including non-PyTorch memory, this process has 23.46 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:46,613 __main__ INFO === Benchmark BlockwiseCheckpointedTransformer memory ===
2025-12-04 20:37:46,613 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=xlarge, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1536, 'num_heads': 24, 'd_ff': 6144}, batch_size=4, seq_len=256, transformer_cls=BlockwiseCheckpointedTransformer
2025-12-04 20:37:46,667 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24970788864, max_allocated_bytes=23746573312, max_reserved_bytes=24970788864)
2025-12-04 20:37:48,565 __main__ INFO OOM? CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 21.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 22.22 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-12-04 20:37:48,568 __main__ INFO === Benchmark Transformer memory ===
2025-12-04 20:37:48,568 __main__ INFO Profile memory for Transformer activation checkpointing: model_size=xlarge, model_config={'vocab_size': 50257, 'num_layers': 48, 'd_model': 1536, 'num_heads': 24, 'd_ff': 6144}, batch_size=4, seq_len=256, transformer_cls=Transformer
2025-12-04 20:37:48,568 __main__ INFO Initial memory usage: MemoryStats(curr_allocated_bytes=17039360, curr_reserved_bytes=24949817344, max_allocated_bytes=23859819520, max_reserved_bytes=24970788864)
2025-12-04 20:37:50,026 __main__ INFO OOM? CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 17.88 MiB is free. Including non-PyTorch memory, this process has 23.44 GiB memory in use. Of the allocated memory 19.97 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
